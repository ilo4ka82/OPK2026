"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings (–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π) —Ç–µ–∫—Å—Ç–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
"""
import logging
from typing import List
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class EmbeddingModel:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self, model_name: str = "intfloat/multilingual-e5-large"):
        """
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace
                       –í–∞—Ä–∏–∞–Ω—Ç—ã:
                       - "intfloat/multilingual-e5-large" (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è, 560MB)
                       - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2" (–ª–µ–≥—á–µ, 278MB)
        """
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ embeddings: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.model.get_sentence_embedding_dimension()}")
    
    def embed_text(self, text: str) -> List[float]:
        """
        –°–æ–∑–¥–∞—ë—Ç embedding –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ float (–≤–µ–∫—Ç–æ—Ä)
        """
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding.tolist()
    
    def embed_texts(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> List[List[float]]:
        """
        –°–æ–∑–¥–∞—ë—Ç embeddings –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (–±–∞—Ç—á–∞–º–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} embeddings")
        return embeddings.tolist()
    
    def get_dimension(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞"""
        return self.model.get_sentence_embedding_dimension()


# === –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    embedder = EmbeddingModel()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
    test_texts = [
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è?",
        "–ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏—ë–º–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç 2025 –≥–æ–¥–∞",
        "–ë–í–ò - –±–µ–∑ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π"
    ]
    
    # –°–æ–∑–¥–∞—ë–º embeddings
    embeddings = embedder.embed_texts(test_texts)
    
    print(f"\nüìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {embedder.get_dimension()}")
    print(f"üìù –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} embeddings")
    print(f"üìÑ –ü—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ (–ø–µ—Ä–≤—ã–µ 10 —á–∏—Å–µ–ª): {embeddings[0][:10]}")